{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will copy paste the inital batch of inputs and weigths and add the new weights and biases to create a new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1,2,3,2.5],\n",
    "          [2.,5.,-1.,2.],\n",
    "          [-1.5,2.7,3.3,-0.8]]\n",
    "weights = [[0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87]]\n",
    "biases =  [2,3,0.5] \n",
    "\n",
    "weights2 = [[0.1,-0.14,0.5],\n",
    "            [-0.5,0.12,-0.33],\n",
    "            [-0.44,0.73,-0.13]]\n",
    "biases2 = [-1,2,-0.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_outputs = np.dot(inputs,np.array(weights).T) + biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, our neural network should look like:**\n",
    "![alt text](neurons_with_2_hidden_layers.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nnfs.init() does three things: it sets the random seed to 0(by the default),creates a float32 dtype default, and overrides the original dot produt from NumPy. All of these are meant to ensure repeatable results for following along. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Dense Layer \n",
    "class Layer_Dense: \n",
    "    #Layer initialization \n",
    "    def __init__(self,n_inputs, n_neurons):\n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #Forward pass \n",
    "    def forward(self,inputs):\n",
    "         # Calculate out values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "# Create a dataset \n",
    "X,y = spiral_data(samples = 100,classes =3)\n",
    "#Create a Dense Layer with with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2,3)\n",
    "dense1.forward(X)\n",
    "# Let's see output of the first few samples: \n",
    "print(dense1.output[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100] \n",
    "\n",
    "output = [] \n",
    "for i in inputs: \n",
    "    if i > 0: \n",
    "        output.append(i)\n",
    "    else: \n",
    "        output.append(0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "inputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100] \n",
    "output = np.maximum(0,inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU: \n",
    "    #Forward pass \n",
    "    def forward(self,inputs):\n",
    "        # Calculate output values from input \n",
    "        self.output = np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100,classes=3)\n",
    "\n",
    "#Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# Create ReLU activation ( to be used with Dense Layer): \n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of our training data through this layer \n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation func. \n",
    "#Takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "#Let's see output of the first few samples \n",
    "print(activation1.output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8,1,21,2,385]\n",
    "e = 2.71828182846 #math.e can also be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751893969, 2.71828182846, 1318815734.4929411, 7.38905609893584, 1.5972596947447657e+167]\n"
     ]
    }
   ],
   "source": [
    "exp_values = [] \n",
    "for output in layer_outputs: \n",
    "    exp_values.append(e**output) # ** - power operator in python\n",
    "print(\"exponentiated values:\") \n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values:\n",
      "[7.607430270652166e-166, 1.7018408699622063e-167, 8.256739582373807e-159, 4.626083111748822e-167, 1.0]\n",
      "Sum of normalized values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Now normalize values \n",
    "norm_base = sum(exp_values) \n",
    "norm_values = [] \n",
    "for value in exp_values: \n",
    "    norm_values.append(value/norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "\n",
    "print('Sum of normalized values:',sum(norm_values)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things can be done in numpy in following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21,2.385] \n",
    "# For each value in a vector, calculate the exponential value \n",
    "exp_values = np.exp(layer_outputs) \n",
    "print('exponentiated values:') \n",
    "print( exp_values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized exponential values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#Now normalize values \n",
    "norm_values = exp_values / np.sum(exp_values) \n",
    "print('normalized exponential values:') \n",
    "print(norm_values) \n",
    "print('sum of normalized values:',np.sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
