{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will copy paste the inital batch of inputs and weigths and add the new weights and biases to create a new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1,2,3,2.5],\n",
    "          [2.,5.,-1.,2.],\n",
    "          [-1.5,2.7,3.3,-0.8]]\n",
    "weights = [[0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87]]\n",
    "biases =  [2,3,0.5] \n",
    "\n",
    "weights2 = [[0.1,-0.14,0.5],\n",
    "            [-0.5,0.12,-0.33],\n",
    "            [-0.44,0.73,-0.13]]\n",
    "biases2 = [-1,2,-0.5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_outputs = np.dot(inputs,np.array(weights).T) + biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, our neural network should look like:**\n",
    "![alt text](neurons_with_2_hidden_layers.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nnfs.init() does three things: it sets the random seed to 0(by the default),creates a float32 dtype default, and overrides the original dot produt from NumPy. All of these are meant to ensure repeatable results for following along. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Dense Layer \n",
    "class Layer_Dense: \n",
    "    #Layer initialization \n",
    "    def __init__(self,n_inputs, n_neurons):\n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    #Forward pass \n",
    "    def forward(self,inputs):\n",
    "         # Calculate out values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "# Create a dataset \n",
    "X,y = spiral_data(samples = 100,classes =3)\n",
    "#Create a Dense Layer with with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2,3)\n",
    "dense1.forward(X)\n",
    "# Let's see output of the first few samples: \n",
    "print(dense1.output[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100] \n",
    "\n",
    "output = [] \n",
    "for i in inputs: \n",
    "    if i > 0: \n",
    "        output.append(i)\n",
    "    else: \n",
    "        output.append(0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "inputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100] \n",
    "output = np.maximum(0,inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU: \n",
    "    #Forward pass \n",
    "    def forward(self,inputs):\n",
    "        # Calculate output values from input \n",
    "        self.output = np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100,classes=3)\n",
    "\n",
    "#Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# Create ReLU activation ( to be used with Dense Layer): \n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of our training data through this layer \n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation func. \n",
    "#Takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "#Let's see output of the first few samples \n",
    "print(activation1.output[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8,1,21,2,385]\n",
    "e = 2.71828182846 #math.e can also be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751893969, 2.71828182846, 1318815734.4929411, 7.38905609893584, 1.5972596947447657e+167]\n"
     ]
    }
   ],
   "source": [
    "exp_values = [] \n",
    "for output in layer_outputs: \n",
    "    exp_values.append(e**output) # ** - power operator in python\n",
    "print(\"exponentiated values:\") \n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values:\n",
      "[7.607430270652166e-166, 1.7018408699622063e-167, 8.256739582373807e-159, 4.626083111748822e-167, 1.0]\n",
      "Sum of normalized values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Now normalize values \n",
    "norm_base = sum(exp_values) \n",
    "norm_values = [] \n",
    "for value in exp_values: \n",
    "    norm_values.append(value/norm_base)\n",
    "print('Normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "\n",
    "print('Sum of normalized values:',sum(norm_values)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things can be done in numpy in following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21,2.385] \n",
    "# For each value in a vector, calculate the exponential value \n",
    "exp_values = np.exp(layer_outputs) \n",
    "print('exponentiated values:') \n",
    "print( exp_values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized exponential values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#Now normalize values \n",
    "norm_values = exp_values / np.sum(exp_values) \n",
    "print('normalized exponential values:') \n",
    "print(norm_values) \n",
    "print('sum of normalized values:',np.sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code + Additional code upto this point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nnfs \n",
    "from nnfs.datasets import spiral_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense Layer \n",
    "class Layer_Dense:\n",
    "    #Layer initilization \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #Initialize weights and biases \n",
    "        self.weights = 0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    # Forward pass\n",
    "    def forward(self,inputs):\n",
    "        # Calculate output values from inputs, weight and biases \n",
    "        self.output = np.dot(inputs,self.weights) + self.biases \n",
    "\n",
    "# Activation ReLU \n",
    "class Activation_ReLU: \n",
    "     #Forward pass \n",
    "     def forward(self,inputs): \n",
    "         # Calculate ouput values from inputs \n",
    "        self.output = np.maximum(0,inputs) \n",
    "class Activation_softmax: \n",
    "        # Forward pass \n",
    "        def forward(self, inputs):\n",
    "\n",
    "            #Get unnormalized probabilities \n",
    "            exp_values = np.exp(inputs - np.max(inputs, axis =1,\n",
    "                                                keepdims = True))\n",
    "\n",
    "            # Normalize them for each sample \n",
    "            probabilities = exp_values / np.sum(exp_values , axis=1,\n",
    "                                               keepdims=True)\n",
    "            self.output = probabilities\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "X,y = spiral_data(samples = 100, classes = 3) \n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2,3)\n",
    "# Create ReLU activation (to be used with Dense layer ) \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "#Create second dense layer with 3 input features ( as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "#Create Softmax activation ( to be used with Dense Layer) \n",
    "activation2 = Activation_softmax()\n",
    "\n",
    "# Make a  forward pass of our training data through the first layer \n",
    "dense1.forward(X)  \n",
    "\n",
    "# make a forward pass through activation function \n",
    "# it takes the output of first dense layer here \n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# it takes output of activation function of first layer as inputs\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through 2nd activation function \n",
    "# It takes the output of second dense layer here \n",
    "activation2.forward(dense2.output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333355 0.33333337 0.33333308]\n",
      " [0.3333336  0.33333415 0.33333224]\n",
      " [0.33333397 0.33333427 0.33333182]\n",
      " [0.33333388 0.33333504 0.33333114]\n",
      " [0.33333412 0.33333528 0.33333057]\n",
      " [0.33333308 0.33333614 0.33333078]\n",
      " [0.33333337 0.33333647 0.33333018]\n",
      " [0.3333332  0.333337   0.3333298 ]\n",
      " [0.33333415 0.33333734 0.3333285 ]\n",
      " [0.33333212 0.33333832 0.3333296 ]\n",
      " [0.33332893 0.33333874 0.33333233]\n",
      " [0.33333388 0.33333877 0.33332735]\n",
      " [0.3333308  0.33334    0.33332914]\n",
      " [0.33333462 0.33333957 0.3333258 ]\n",
      " [0.33332664 0.33334157 0.33333176]\n",
      " [0.33333012 0.33334154 0.33332837]\n",
      " [0.33332607 0.3333423  0.3333316 ]\n",
      " [0.3333336  0.33334124 0.33332512]\n",
      " [0.33332497 0.33334368 0.3333314 ]\n",
      " [0.33332607 0.33334303 0.3333309 ]\n",
      " [0.33332366 0.3333453  0.33333108]\n",
      " [0.33332333 0.33334565 0.33333102]\n",
      " [0.33332315 0.3333459  0.33333096]\n",
      " [0.3333253  0.33334327 0.33333144]\n",
      " [0.33332542 0.33334595 0.3333286 ]\n",
      " [0.33332172 0.33334768 0.33333063]\n",
      " [0.33332354 0.33334538 0.33333105]\n",
      " [0.33332092 0.3333486  0.33333045]\n",
      " [0.33332074 0.33334884 0.33333042]\n",
      " [0.33332083 0.33334878 0.33333042]\n",
      " [0.33331895 0.33335108 0.33332998]\n",
      " [0.33332244 0.33334678 0.3333308 ]\n",
      " [0.3333262  0.33334216 0.33333167]\n",
      " [0.3333243  0.33334443 0.33333123]\n",
      " [0.33332315 0.3333459  0.33333096]\n",
      " [0.33332276 0.33334637 0.3333309 ]\n",
      " [0.33332545 0.3333431  0.3333315 ]\n",
      " [0.33333695 0.33332956 0.3333335 ]\n",
      " [0.33331656 0.33335397 0.33332944]\n",
      " [0.33333343 0.33333328 0.33333328]\n",
      " [0.33333147 0.33333564 0.3333329 ]\n",
      " [0.3333408  0.33332556 0.3333337 ]\n",
      " [0.3333366  0.33332995 0.33333352]\n",
      " [0.33334234 0.3333273  0.3333303 ]\n",
      " [0.33332166 0.3333477  0.33333063]\n",
      " [0.3333409  0.3333254  0.33333367]\n",
      " [0.33334115 0.33332515 0.33333367]\n",
      " [0.33334452 0.33333442 0.33332103]\n",
      " [0.33333457 0.33333212 0.3333334 ]\n",
      " [0.33334222 0.33332404 0.33333373]\n",
      " [0.33334377 0.33332634 0.3333299 ]\n",
      " [0.33334276 0.33332345 0.33333373]\n",
      " [0.33334547 0.33334416 0.33331037]\n",
      " [0.333345   0.3333283  0.33332676]\n",
      " [0.33334178 0.3333553  0.33330294]\n",
      " [0.33334512 0.3333495  0.33330542]\n",
      " [0.333345   0.33335078 0.3333043 ]\n",
      " [0.33334577 0.3333491  0.33330506]\n",
      " [0.33334672 0.3333316  0.33332166]\n",
      " [0.3333474  0.33334273 0.33330992]\n",
      " [0.333344   0.3333559  0.33330014]\n",
      " [0.33334252 0.33335847 0.33329904]\n",
      " [0.3333372  0.33336183 0.33330098]\n",
      " [0.33334643 0.33335283 0.33330068]\n",
      " [0.33334732 0.333351   0.3333017 ]\n",
      " [0.33333614 0.3333632  0.33330062]\n",
      " [0.3333459  0.3333565  0.33329755]\n",
      " [0.33333227 0.3333644  0.3333033 ]\n",
      " [0.33332357 0.33336803 0.33330837]\n",
      " [0.33333245 0.3333652  0.33330232]\n",
      " [0.33333802 0.33336538 0.3332965 ]\n",
      " [0.33333266 0.33336604 0.33330134]\n",
      " [0.3333197  0.3333707  0.33330968]\n",
      " [0.333326   0.33336967 0.33330432]\n",
      " [0.3332987  0.333376   0.33332527]\n",
      " [0.3333355  0.3333671  0.33329743]\n",
      " [0.33333322 0.33336797 0.33329883]\n",
      " [0.33332896 0.33337042 0.33330068]\n",
      " [0.33331802 0.3333738  0.3333082 ]\n",
      " [0.3333387  0.33336946 0.33329183]\n",
      " [0.33331138 0.3333749  0.3333137 ]\n",
      " [0.33330828 0.33337492 0.3333168 ]\n",
      " [0.3332973  0.33337775 0.33332497]\n",
      " [0.33329448 0.33338124 0.3333243 ]\n",
      " [0.33330706 0.33337635 0.33331653]\n",
      " [0.3332958  0.33337963 0.3333246 ]\n",
      " [0.33330825 0.33336425 0.3333275 ]\n",
      " [0.33329242 0.33338374 0.33332384]\n",
      " [0.33331063 0.33336133 0.33332807]\n",
      " [0.33329487 0.33338076 0.33332437]\n",
      " [0.3333261  0.33334228 0.33333167]\n",
      " [0.3333126  0.3333589  0.33332852]\n",
      " [0.33329082 0.33338577 0.33332342]\n",
      " [0.3333004  0.33337393 0.33332568]\n",
      " [0.33332303 0.333346   0.33333093]\n",
      " [0.3333065  0.33336645 0.33332708]\n",
      " [0.3333087  0.33336374 0.33332762]\n",
      " [0.3333383  0.33332816 0.33333355]\n",
      " [0.33330324 0.33337045 0.33332637]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333325 0.33333337 0.33333334]\n",
      " [0.33333302 0.3333337  0.33333325]\n",
      " [0.33333325 0.33333343 0.3333333 ]\n",
      " [0.33333376 0.3333329  0.33333337]\n",
      " [0.33333376 0.33333287 0.3333333 ]\n",
      " [0.33333287 0.33333385 0.33333325]\n",
      " [0.3333327  0.33333415 0.3333332 ]\n",
      " [0.33333352 0.3333331  0.33333334]\n",
      " [0.33333445 0.33333215 0.3333334 ]\n",
      " [0.3333346  0.33333197 0.3333334 ]\n",
      " [0.33333427 0.33333236 0.33333337]\n",
      " [0.3333348  0.33333176 0.33333337]\n",
      " [0.33333406 0.3333326  0.33333334]\n",
      " [0.3333364  0.33333236 0.3333312 ]\n",
      " [0.33333647 0.33333144 0.33333212]\n",
      " [0.33333555 0.33333105 0.33333343]\n",
      " [0.3333372  0.333333   0.3333298 ]\n",
      " [0.33333707 0.33333865 0.33332428]\n",
      " [0.3333374  0.3333314  0.3333312 ]\n",
      " [0.33333698 0.33334044 0.33332258]\n",
      " [0.3333383  0.33333507 0.33332664]\n",
      " [0.33333823 0.3333385  0.33332327]\n",
      " [0.33333838 0.33333918 0.3333224 ]\n",
      " [0.3333357  0.33334392 0.33332032]\n",
      " [0.33333817 0.3333417  0.33332014]\n",
      " [0.33333945 0.33333656 0.33332393]\n",
      " [0.3333397  0.33333692 0.33332342]\n",
      " [0.33333993 0.3333369  0.33332315]\n",
      " [0.33334017 0.33333716 0.33332264]\n",
      " [0.33333677 0.3333463  0.33331686]\n",
      " [0.3333391  0.3333441  0.33331674]\n",
      " [0.33333609 0.33334768 0.33331627]\n",
      " [0.3333376  0.3333473  0.3333151 ]\n",
      " [0.33333728 0.33334804 0.3333147 ]\n",
      " [0.33333552 0.33334917 0.3333153 ]\n",
      " [0.33332604 0.3333518  0.33332214]\n",
      " [0.33332253 0.3333522  0.33332524]\n",
      " [0.33332506 0.3333529  0.33332208]\n",
      " [0.33333442 0.3333509  0.3333147 ]\n",
      " [0.3333194  0.33335304 0.3333276 ]\n",
      " [0.33331746 0.33335292 0.33332962]\n",
      " [0.3333184  0.33335388 0.33332765]\n",
      " [0.33332363 0.3333555  0.33332095]\n",
      " [0.33331364 0.3333576  0.33332875]\n",
      " [0.33331347 0.33335784 0.33332872]\n",
      " [0.3333383  0.33335346 0.33330825]\n",
      " [0.3333115  0.33336025 0.33332825]\n",
      " [0.33332565 0.33335766 0.33331665]\n",
      " [0.33331105 0.3333608  0.3333282 ]\n",
      " [0.33331022 0.3333618  0.33332795]\n",
      " [0.3333149  0.33335608 0.33332902]\n",
      " [0.33330938 0.33336285 0.33332777]\n",
      " [0.33330902 0.3333633  0.33332768]\n",
      " [0.33330947 0.3333628  0.33332777]\n",
      " [0.33331984 0.33334997 0.3333302 ]\n",
      " [0.33331183 0.33335984 0.33332837]\n",
      " [0.33331192 0.33335972 0.33332837]\n",
      " [0.33332258 0.33334664 0.33333084]\n",
      " [0.3333184  0.33335176 0.3333299 ]\n",
      " [0.33331797 0.33335227 0.33332977]\n",
      " [0.33333677 0.33332977 0.3333335 ]\n",
      " [0.33333927 0.33332708 0.33333358]\n",
      " [0.33333457 0.33333212 0.3333334 ]\n",
      " [0.33332402 0.33334488 0.33333117]\n",
      " [0.33334276 0.33332345 0.33333373]\n",
      " [0.3333423  0.33332393 0.33333376]\n",
      " [0.33334088 0.33332542 0.33333367]\n",
      " [0.3333216  0.33334783 0.3333306 ]\n",
      " [0.33334    0.3333264  0.33333364]\n",
      " [0.3333351  0.3333315  0.33333343]\n",
      " [0.33334088 0.33332548 0.33333367]\n",
      " [0.33334467 0.33332145 0.33333382]\n",
      " [0.33334517 0.33332095 0.33333388]\n",
      " [0.33334315 0.33332312 0.33333376]\n",
      " [0.33335102 0.33333853 0.33331046]\n",
      " [0.33333737 0.3333291  0.3333335 ]\n",
      " [0.33335137 0.33333626 0.33331236]\n",
      " [0.33333942 0.33332697 0.3333336 ]\n",
      " [0.33335194 0.33333734 0.33331078]\n",
      " [0.33334854 0.33331853 0.33333296]\n",
      " [0.33335134 0.3333529  0.3332958 ]\n",
      " [0.33335268 0.33334312 0.33330414]\n",
      " [0.33335242 0.33334962 0.33329794]\n",
      " [0.33335233 0.33333018 0.33331755]\n",
      " [0.3333534  0.33334094 0.3333056 ]\n",
      " [0.33335102 0.33335936 0.33328965]\n",
      " [0.33334762 0.33336693 0.3332855 ]\n",
      " [0.33331242 0.33337864 0.33330896]\n",
      " [0.3333476  0.33336815 0.33328426]\n",
      " [0.33333892 0.33337402 0.33328703]\n",
      " [0.33335462 0.33334765 0.3332977 ]\n",
      " [0.33335376 0.33335578 0.3332905 ]\n",
      " [0.33334628 0.3333719  0.33328184]\n",
      " [0.3333514  0.33336496 0.3332837 ]\n",
      " [0.33331135 0.33338225 0.33330646]\n",
      " [0.3333412  0.3333764  0.3332824 ]\n",
      " [0.33330494 0.3333828  0.33331224]\n",
      " [0.33329922 0.33338162 0.33331913]\n",
      " [0.33332467 0.3333816  0.33329377]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333322 0.33333382 0.33333293]\n",
      " [0.33333275 0.33333433 0.33333287]\n",
      " [0.3333324  0.33333483 0.33333278]\n",
      " [0.3333324  0.3333354  0.3333322 ]\n",
      " [0.33333138 0.3333357  0.3333329 ]\n",
      " [0.33333054 0.33333674 0.3333327 ]\n",
      " [0.3333302  0.3333372  0.3333326 ]\n",
      " [0.3333299  0.33333758 0.33333254]\n",
      " [0.33332944 0.3333381  0.33333242]\n",
      " [0.33333036 0.33333844 0.33333123]\n",
      " [0.33332968 0.33333883 0.3333315 ]\n",
      " [0.33332777 0.33334023 0.33333206]\n",
      " [0.33332887 0.33333978 0.33333138]\n",
      " [0.33332714 0.3333409  0.33333188]\n",
      " [0.33332846 0.3333393  0.3333322 ]\n",
      " [0.33333078 0.33333647 0.33333272]\n",
      " [0.33332628 0.33334202 0.3333317 ]\n",
      " [0.33333093 0.3333363  0.33333278]\n",
      " [0.33332542 0.3333431  0.3333315 ]\n",
      " [0.33332598 0.33334237 0.3333316 ]\n",
      " [0.33332592 0.33334243 0.33333158]\n",
      " [0.33333167 0.33333534 0.33333293]\n",
      " [0.3333228  0.3333463  0.33333087]\n",
      " [0.33333033 0.333337   0.33333263]\n",
      " [0.33332446 0.3333443  0.33333126]\n",
      " [0.33333343 0.33333328 0.3333333 ]\n",
      " [0.3333368  0.33332968 0.3333335 ]\n",
      " [0.33333206 0.33333486 0.33333302]\n",
      " [0.3333349  0.3333317  0.3333334 ]\n",
      " [0.3333357  0.33333087 0.33333343]\n",
      " [0.33333054 0.33333674 0.3333327 ]\n",
      " [0.3333397  0.33332828 0.33333197]\n",
      " [0.3333406  0.33333102 0.33332837]\n",
      " [0.3333392  0.33332717 0.33333358]\n",
      " [0.33333826 0.33332816 0.33333352]\n",
      " [0.33333737 0.3333291  0.3333335 ]\n",
      " [0.3333376  0.33332887 0.33333352]\n",
      " [0.33334225 0.33333927 0.33331853]\n",
      " [0.3333425  0.33333862 0.33331886]\n",
      " [0.3333414  0.33332738 0.3333312 ]\n",
      " [0.3333375  0.33332896 0.33333355]\n",
      " [0.3333314  0.3333357  0.3333329 ]\n",
      " [0.33334088 0.33332548 0.33333367]\n",
      " [0.3333436  0.33333406 0.33332238]\n",
      " [0.33334154 0.33334935 0.33330908]\n",
      " [0.33334392 0.3333329  0.33332324]\n",
      " [0.3333434  0.33334634 0.33331028]\n",
      " [0.33334002 0.33335325 0.33330676]\n",
      " [0.33334368 0.33334714 0.33330914]\n",
      " [0.3333441  0.33334678 0.33330908]\n",
      " [0.33334535 0.33333692 0.33331773]\n",
      " [0.33334556 0.3333405  0.33331394]\n",
      " [0.33334005 0.33335584 0.33330408]\n",
      " [0.33334225 0.3333541  0.33330366]\n",
      " [0.3333397  0.3333572  0.33330315]\n",
      " [0.33333963 0.33335772 0.33330268]\n",
      " [0.33333424 0.33335844 0.3333073 ]\n",
      " [0.33334178 0.333357   0.33330122]\n",
      " [0.33333796 0.33335987 0.33330223]\n",
      " [0.33331084 0.33336204 0.33332708]\n",
      " [0.33333853 0.33336067 0.33330086]\n",
      " [0.33333334 0.33336115 0.33330548]\n",
      " [0.33333495 0.33336073 0.33330426]\n",
      " [0.3333168  0.33336625 0.333317  ]\n",
      " [0.33330774 0.33336487 0.33332735]\n",
      " [0.33332404 0.3333665  0.3333094 ]\n",
      " [0.33331984 0.3333677  0.33331245]\n",
      " [0.33331457 0.33336818 0.33331725]\n",
      " [0.33330384 0.33336967 0.3333265 ]\n",
      " [0.3333148  0.33336926 0.33331588]\n",
      " [0.33330727 0.33336768 0.33332506]\n",
      " [0.3333346  0.333365   0.33330038]\n",
      " [0.33331454 0.33337083 0.33331457]\n",
      " [0.3332998  0.3333747  0.33332556]\n",
      " [0.33330107 0.3333731  0.33332583]\n",
      " [0.33329865 0.3333761  0.33332527]\n",
      " [0.3333005  0.33337379 0.33332568]\n",
      " [0.33330083 0.3333734  0.33332577]\n",
      " [0.33329996 0.33337444 0.33332556]\n",
      " [0.33329886 0.33337584 0.3333253 ]\n",
      " [0.33331552 0.3333553  0.3333292 ]\n",
      " [0.33330366 0.3333699  0.33332643]\n",
      " [0.3332948  0.33338076 0.33332437]\n",
      " [0.33329687 0.33337826 0.33332485]\n",
      " [0.33330044 0.33337387 0.33332565]\n",
      " [0.33330184 0.33337215 0.33332604]\n",
      " [0.33329618 0.33337912 0.33332467]\n",
      " [0.3333053  0.33336794 0.33332682]\n",
      " [0.33331132 0.3333604  0.33332822]\n",
      " [0.3333143  0.3333568  0.3333289 ]\n",
      " [0.3333446  0.33332154 0.33333385]\n",
      " [0.33333722 0.33332926 0.3333335 ]\n",
      " [0.3333201  0.33334962 0.33333024]\n",
      " [0.33333695 0.33332956 0.3333335 ]\n",
      " [0.3333312  0.333336   0.3333328 ]\n",
      " [0.33334142 0.33332488 0.3333337 ]\n",
      " [0.3333522  0.33331665 0.33333117]\n",
      " [0.3333497  0.3333162  0.33333406]\n",
      " [0.33335567 0.33335552 0.33328882]]\n"
     ]
    }
   ],
   "source": [
    "# Let's see output of the first few samples: \n",
    "print(activation2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
